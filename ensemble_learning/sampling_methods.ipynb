{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Dans cette partie, nous allons tester deux méthodes de sampling afin d'entraîner les jeux de données déséquilibrés.\n",
    "\n",
    "- La méthode d'oversampling aléatoire:  on duplique les exemples minoritaires de façon aléatoire jusqu'à ce qu'on atteigne un target_ratio. Elle est plus simple à implémenter et plus compréhensive.\n",
    "- La méthode d'oversampling SMOTE (Synthetic Minority Oversampling Technique): elle est plus complexe car utilise l'interpolation pour générer de nouveaux exemples synthétiques plus variés et réalistes.\n",
    "\n",
    "Il est important de noter qu'on n'oversample que le train_data pour éviter l'overfitting.\n",
    "\n",
    "En terme de modèles, nous allons tester trois modèles:\n",
    "\n",
    "- la régression logistique pénalisée\n",
    "- les forêts aléatoires\n",
    "- le gradient boosting\n",
    "\n",
    "En terme de métrique, nous allons utiliser le recall car il est plus adapté pour mesurer la capacité à prédire la classe minoritaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scripts.prepdata import data_recovery\n",
    "from scripts.models_implementation import *\n",
    "from scripts.cross_validation import imblearn_cross_validation\n",
    "from scripts.convert_to_latex import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = ['abalone8', 'abalone20', 'abalone17', 'bankmarketing', 'libras', 'pageblocks', 'satimage', 'segmentation',\n",
    "                  'wine4', 'yeast3', 'yeast6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du tableau de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index = datasets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Random Sampling\n",
    "#### 1.1.Régression logistique pénalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = {\n",
    "    \"penalty\": [\"l2\", \"elasticnet\"],  \n",
    "    \"C\": [0.1, 1.0, 10.0],\n",
    "    \"solver\": [\"saga\"],                \n",
    "    \"l1_ratio\": [0.5]                  \n",
    "}\n",
    "logreg_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    logreg_list.append(imblearn_cross_validation(x, y, logreg_grid, make_penalized_logreg, \"random\"))\n",
    "results_df[\"rdm logreg\"] = logreg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.Forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [None, 10],         \n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    rf_list.append(imblearn_cross_validation(x, y, rf_grid, make_random_forest, \"random\"))\n",
    "results_df[\"rdm rf\"] = rf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.1, 0.05],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"subsample\": [1.0, 0.8]   \n",
    "}\n",
    "gb_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    gb_list.append(imblearn_cross_validation(x, y, gb_grid, make_gradient_boosting, \"random\"))\n",
    "results_df[\"rdm gb\"] = gb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.SMOTE method\n",
    "#### 2.1 Régression logistique pénalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = {\n",
    "    \"penalty\": [\"l2\", \"elasticnet\"],   \n",
    "    \"C\": [0.1, 1.0, 10.0],\n",
    "    \"solver\": [\"saga\"],                \n",
    "    \"l1_ratio\": [0.5]                  \n",
    "}\n",
    "logreg_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    logreg_list.append(imblearn_cross_validation(x, y, logreg_grid, make_penalized_logreg, \"smote\"))\n",
    "results_df[\"smote logreg\"] = logreg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [None, 10],       \n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    rf_list.append(imblearn_cross_validation(x, y, rf_grid, make_random_forest, \"smote\"))\n",
    "results_df[\"smote rf\"] = rf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.1, 0.05],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"subsample\": [1.0, 0.8]   \n",
    "}\n",
    "gb_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    gb_list.append(imblearn_cross_validation(x, y, gb_grid, make_gradient_boosting, \"smote\"))\n",
    "results_df[\"smote gb\"] = gb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion du tableau de résultats en tableau latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fichier supprimé : sampling_methods_results.pdf\n",
      "✅ Compilation réussie → sampling_methods_results.pdf\n"
     ]
    }
   ],
   "source": [
    "results_df.index.name = \"Dataset\"\n",
    "results_df = results_df.reset_index()\n",
    "results_df_latex = results_to_latex(results_df, caption=\"Recall -- Sampling method\")\n",
    "remove_pdf_if_exists(\"docs/sampling_methods_results.pdf\")\n",
    "inject_table_in_template(results_df_latex, output_pdf=\"docs/sampling_methods_results.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
