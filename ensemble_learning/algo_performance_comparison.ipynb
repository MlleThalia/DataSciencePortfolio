{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Dans cette partie nous allons tester six modèles sur nos datasets afin de comparer leur performance.\n",
    "Les six modèles sont: \n",
    " - la régression logistique pénalisée\n",
    " - Une méthode ensembliste basée sur du bagging\n",
    " - les forêts aléatoires\n",
    " - Une méthode ensembliste basée sur du boosting\n",
    " - une méthode ensembliste basée sur du stacking\n",
    " - le gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scripts.prepdata import data_recovery\n",
    "from scripts.models_implementation import *\n",
    "from scripts.cross_validation import cross_validation\n",
    "from scripts.convert_to_latex import results_to_latex, inject_table_in_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_list = [\"abalone20\", \"abalone17\", \"yeast6\", \"wine4\", \"libras\", \"pageblocks\", \"yeast3\", \"abalone8\", \"segmentation\",\n",
    "                 \"hayes\", \"vehicle\", \"german\", \"glass\", \"wine\", \"pima\", \"iono\", \"autompg\", \"balance\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du tableau de résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(index = datasets_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.La régression logistique pénalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = {\n",
    "    \"penalty\": [\"l2\", \"elasticnet\"],   # 'l1' possible mais ici on garde l2+elasticnet\n",
    "    \"C\": [0.1, 1.0, 10.0],\n",
    "    \"solver\": [\"saga\"],                # saga supporte elasticnet\n",
    "    \"l1_ratio\": [0.5]                  # utilisé seulement si penalty='elasticnet'\n",
    "}\n",
    "logreg_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    logreg_list.append(cross_validation(x, y, logreg_grid, make_penalized_logreg))\n",
    "results_df[\"logreg\"] = logreg_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Méthode ensembliste basée sur du bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_grid = {\n",
    "    \"n_estimators\": [10, 50],\n",
    "    \"max_samples\": [0.7, 1.0],\n",
    "    \"max_features\": [0.7, 1.0],\n",
    "    \"bootstrap\": [True]   # fixe à True par défaut\n",
    "}\n",
    "bagging_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    bagging_list.append(cross_validation(x, y, bagging_grid, make_bagging))\n",
    "results_df[\"bagging\"] = bagging_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Les forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [None, 10],         # None = pas de limite\n",
    "    \"min_samples_leaf\": [1, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "rf_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    rf_list.append(cross_validation(x, y, rf_grid, make_random_forest))\n",
    "results_df[\"randomforest\"] = rf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Méthode ensembliste basée sur du boosting: Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.5, 1.0],\n",
    "}\n",
    "adaboost_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    adaboost_list.append(cross_validation(x, y, adaboost_grid, make_adaboost))\n",
    "results_df[\"adaboost\"] = adaboost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Méthode ensembliste basée sur du stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_grid = {\n",
    "    \"cv\": [3, 5],\n",
    "    \"passthrough\": [False, True]\n",
    "}\n",
    "stacking_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    stacking_list.append(cross_validation(x, y, stacking_grid, make_stacking))\n",
    "results_df[\"stacking\"] = stacking_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Le gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"learning_rate\": [0.1, 0.05],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"subsample\": [1.0, 0.8]   # 0.8 = léger bagging, utile pour régulariser\n",
    "}\n",
    "gb_list = []\n",
    "for dataset_name in datasets_list:\n",
    "    dataset = data_recovery(dataset_name)\n",
    "    x,y = dataset\n",
    "    gb_list.append(cross_validation(x, y, gb_grid, make_gradient_boosting))\n",
    "results_df[\"gradientboosting\"] = gb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion du tableau de résultats en tableau latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Compilation réussie → algo_performance_results.pdf\n"
     ]
    }
   ],
   "source": [
    "results_df.index.name = \"Dataset\"\n",
    "results_df = results_df.reset_index()\n",
    "results_df_latex = results_to_latex(results_df)\n",
    "inject_table_in_template(results_df_latex, output_pdf=\"algo_performance_results.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
